{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Bot Battle Arena\n",
    "\n",
    "This notebook runs tournaments between different agent types and visualizes their performance.\n",
    "\n",
    "We'll compare:\n",
    "- **Random Agent**: Makes random legal moves\n",
    "- **RuleBased Agent**: Uses hand evaluation heuristics\n",
    "- **MCTS Agent**: Monte Carlo Tree Search for card play\n",
    "- **CFR Agent**: Counterfactual Regret Minimization for bidding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, os.path.abspath(os.path.join('..')))\n",
    "\n",
    "from euchre.engine.state import EuchreGameState, GamePhase\n",
    "from euchre.agents import RandomAgent, RuleBasedAgent, MCTSAgent\n",
    "\n",
    "# Import the evaluator functions from utils\n",
    "from euchre.utils.evaluator import run_tournament, TournamentStats\n",
    "\n",
    "print('Imports successful!')\n",
    "\n",
    "# Set up matplotlib style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tournament 1: Random vs Random (Control)\n",
    "\n",
    "First, let's establish a baseline by having random agents play against each other.\n",
    "We expect roughly 50/50 win rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Running Random vs Random (100 games)...')\n",
    "team0 = (RandomAgent('R0'), RandomAgent('R2'))\n",
    "team1 = (RandomAgent('R1'), RandomAgent('R3'))\n",
    "stats_random = run_tournament(team0, team1, num_games=100, verbose=False)\n",
    "stats_random.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tournament 2: RuleBased vs Random\n",
    "\n",
    "Now let's see how much better a heuristic-based agent performs compared to random play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Running RuleBased vs Random (100 games)...')\n",
    "team0 = (RuleBasedAgent('H0'), RuleBasedAgent('H2'))\n",
    "team1 = (RandomAgent('R1'), RandomAgent('R3'))\n",
    "stats_heuristic = run_tournament(team0, team1, num_games=100, verbose=False)\n",
    "stats_heuristic.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tournament 3: RuleBased vs RuleBased\n",
    "\n",
    "What happens when two heuristic agents face each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Running RuleBased vs RuleBased (100 games)...')\n",
    "team0 = (RuleBasedAgent('H0'), RuleBasedAgent('H2'))\n",
    "team1 = (RuleBasedAgent('H1'), RuleBasedAgent('H3'))\n",
    "stats_heuristic_mirror = run_tournament(team0, team1, num_games=100, verbose=False)\n",
    "stats_heuristic_mirror.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tournament 4: MCTS vs RuleBased\n",
    "\n",
    "MCTS uses simulation to find good moves. It's slower but should be stronger.\n",
    "\n",
    "**Note:** Running fewer games due to computational cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Running MCTS vs RuleBased (20 games, this will take a while)...')\n",
    "team0 = (MCTSAgent('M0', simulation_time=0.5), MCTSAgent('M2', simulation_time=0.5))\n",
    "team1 = (RuleBasedAgent('H1'), RuleBasedAgent('H3'))\n",
    "stats_mcts = run_tournament(team0, team1, num_games=20, verbose=False)\n",
    "stats_mcts.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 1: Win Rates Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect win rate data\n",
    "matchups = [\n",
    "    'Random vs\\nRandom',\n",
    "    'RuleBased vs\\nRandom',\n",
    "    'RuleBased vs\\nRuleBased',\n",
    "    'MCTS vs\\nRuleBased'\n",
    "]\n",
    "\n",
    "team0_win_rates = [\n",
    "    stats_random.team0_wins / stats_random.games_played * 100,\n",
    "    stats_heuristic.team0_wins / stats_heuristic.games_played * 100,\n",
    "    stats_heuristic_mirror.team0_wins / stats_heuristic_mirror.games_played * 100,\n",
    "    stats_mcts.team0_wins / stats_mcts.games_played * 100\n",
    "]\n",
    "\n",
    "team1_win_rates = [\n",
    "    stats_random.team1_wins / stats_random.games_played * 100,\n",
    "    stats_heuristic.team1_wins / stats_heuristic.games_played * 100,\n",
    "    stats_heuristic_mirror.team1_wins / stats_heuristic_mirror.games_played * 100,\n",
    "    stats_mcts.team1_wins / stats_mcts.games_played * 100\n",
    "]\n",
    "\n",
    "# Create grouped bar chart\n",
    "x = np.arange(len(matchups))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars1 = ax.bar(x - width/2, team0_win_rates, width, label='Team 0 (First)', color='#2E86AB')\n",
    "bars2 = ax.bar(x + width/2, team1_win_rates, width, label='Team 1 (Second)', color='#A23B72')\n",
    "\n",
    "ax.set_ylabel('Win Rate (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Agent Win Rates Across Different Matchups', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(matchups)\n",
    "ax.legend()\n",
    "ax.set_ylim([0, 100])\n",
    "ax.axhline(y=50, color='gray', linestyle='--', alpha=0.5, label='50% baseline')\n",
    "\n",
    "# Add value labels on bars\n",
    "def autolabel(bars):\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.1f}%',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom',\n",
    "                    fontsize=9, fontweight='bold')\n",
    "\n",
    "autolabel(bars1)\n",
    "autolabel(bars2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 2: Average Scores per Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect average score data\n",
    "team0_avg_scores = [\n",
    "    stats_random.team0_total_score / stats_random.games_played,\n",
    "    stats_heuristic.team0_total_score / stats_heuristic.games_played,\n",
    "    stats_heuristic_mirror.team0_total_score / stats_heuristic_mirror.games_played,\n",
    "    stats_mcts.team0_total_score / stats_mcts.games_played\n",
    "]\n",
    "\n",
    "team1_avg_scores = [\n",
    "    stats_random.team1_total_score / stats_random.games_played,\n",
    "    stats_heuristic.team1_total_score / stats_heuristic.games_played,\n",
    "    stats_heuristic_mirror.team1_total_score / stats_heuristic_mirror.games_played,\n",
    "    stats_mcts.team1_total_score / stats_mcts.games_played\n",
    "]\n",
    "\n",
    "# Create grouped bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars1 = ax.bar(x - width/2, team0_avg_scores, width, label='Team 0', color='#2E86AB')\n",
    "bars2 = ax.bar(x + width/2, team1_avg_scores, width, label='Team 1', color='#A23B72')\n",
    "\n",
    "ax.set_ylabel('Average Final Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Average Game Scores by Matchup (Target: 10 points)', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(matchups)\n",
    "ax.legend()\n",
    "ax.axhline(y=10, color='green', linestyle='--', alpha=0.5, label='Target score')\n",
    "\n",
    "# Add value labels\n",
    "def autolabel_scores(bars):\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.1f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom',\n",
    "                    fontsize=9, fontweight='bold')\n",
    "\n",
    "autolabel_scores(bars1)\n",
    "autolabel_scores(bars2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 3: Agent Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary table\n",
    "import pandas as pd\n",
    "\n",
    "summary_data = {\n",
    "    'Matchup': matchups,\n",
    "    'Team 0 Agent': ['Random', 'RuleBased', 'RuleBased', 'MCTS'],\n",
    "    'Team 1 Agent': ['Random', 'Random', 'RuleBased', 'RuleBased'],\n",
    "    'Games Played': [\n",
    "        stats_random.games_played,\n",
    "        stats_heuristic.games_played,\n",
    "        stats_heuristic_mirror.games_played,\n",
    "        stats_mcts.games_played\n",
    "    ],\n",
    "    'Team 0 Win %': [f'{wr:.1f}%' for wr in team0_win_rates],\n",
    "    'Team 1 Win %': [f'{wr:.1f}%' for wr in team1_win_rates],\n",
    "    'Avg Hands/Game': [\n",
    "        stats_random.total_hands / stats_random.games_played,\n",
    "        stats_heuristic.total_hands / stats_heuristic.games_played,\n",
    "        stats_heuristic_mirror.total_hands / stats_heuristic_mirror.games_played,\n",
    "        stats_mcts.total_hands / stats_mcts.games_played\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(summary_data)\n",
    "df['Avg Hands/Game'] = df['Avg Hands/Game'].round(1)\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('TOURNAMENT SUMMARY TABLE')\n",
    "print('='*80)\n",
    "print(df.to_string(index=False))\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "Based on the tournaments above:\n",
    "\n",
    "1. **Random vs Random**: Should show ~50/50 win rate (baseline)\n",
    "2. **RuleBased vs Random**: RuleBased agent should win significantly more often (typically 60-70%)\n",
    "3. **RuleBased vs RuleBased**: Should return to ~50/50 when agents are matched\n",
    "4. **MCTS vs RuleBased**: MCTS should have a slight edge due to better card play decisions\n",
    "\n",
    "### Agent Strengths:\n",
    "\n",
    "- **RandomAgent**: Useful for baseline testing, no strategic value\n",
    "- **RuleBasedAgent**: Strong bidding heuristics, fast execution, good baseline\n",
    "- **MCTSAgent**: Better card play through simulation, slower but more accurate\n",
    "- **CFRAgent**: (If trained) Near-optimal bidding strategy for Round 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Test CFR Agent\n",
    "\n",
    "If you've trained the CFR policy, uncomment and run this cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     from euchre.agents import CFRAgent\n",
    "#     \n",
    "#     print('Running CFR vs RuleBased (100 games)...')\n",
    "#     team0 = (CFRAgent('C0'), CFRAgent('C2'))\n",
    "#     team1 = (RuleBasedAgent('H1'), RuleBasedAgent('H3'))\n",
    "#     stats_cfr = run_tournament(team0, team1, num_games=100, verbose=False)\n",
    "#     stats_cfr.print_summary()\n",
    "# except FileNotFoundError:\n",
    "#     print('CFR policy file not found!')\n",
    "#     print('Train the policy first with: python -m euchre.training.cfr_trainer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Train CFR**: Run `python -m euchre.training.cfr_trainer` to create the policy file\n",
    "2. **Tune MCTS**: Experiment with different `simulation_time` values\n",
    "3. **Improve Heuristics**: Modify the RuleBasedAgent's hand evaluation\n",
    "4. **Build UI**: Create a CLI or web interface to play against the bots"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
